{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 误差反向传播法的目标和理论基础\n",
    "### 目标\n",
    "直接计算梯度的方法，在计算上时间复杂度较高。误差反向传播法能够有效的降低时间复杂度。\n",
    "\n",
    "### 理论基础\n",
    "如果一个函数$h(x)$和函数$g(x)$、$f(x)$满足：\n",
    "$$ h(x)=g(f(x)) $$\n",
    "\n",
    "且再求导数的目标范围内函数连续可导，假设$y=f(x)$则$h(x)$的导数满足：\n",
    "\n",
    "$$\n",
    "\\frac{dh(x)}{dx}=\\frac{dg(y)}{dy}\\cdot\\frac{dy}{dx}\n",
    "$$\n",
    "\n",
    "导数的这一性质，使得导数在计算过程中具有传递的特征，这种特征也被称为**链式法则**。利用这一性质，可将一个复杂的函数表达成简单的复合函数，逐级求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图\n",
    "### 概念\n",
    "计算图就是一个函数传递的过程，将函数通过一个一个的操作符逐级生成。通过单一操作符形成的函数，都可以利用计算图的方式来求解导数或者梯度。\n",
    "\n",
    "![image](http://www.ituring.com.cn/figures/2018/DeepLearning/060.png)\n",
    "\n",
    "这一构建计算图的过程称为“正向传播”。而通过结果，求解每一级的导数的过程，称为“反向传播”。\n",
    "\n",
    "### 优势\n",
    "利用计算图的概念，每次计算导数的时候，可以只计算局部的导数。利用链式法则（上文中复合函数的导数性质），最终可以构成完整的函数导数。\n",
    "\n",
    "![image](http://www.ituring.com.cn/figures/2018/DeepLearning/065.png)\n",
    "\n",
    "### 几个常见操作符的导数性质\n",
    "#### 加法节点\n",
    "以$z=x+y$为例，$\\frac{\\partial z}{\\partial x}=\\frac{\\partial z}{\\partial y}=1$。因此，加法节点需要传播的都是1。\n",
    "\n",
    "#### 乘法节点\n",
    "\n",
    "以$z=xy$为例，$\\frac{\\partial z}{\\partial x}=y$`,`$\\frac{\\partial z}{\\partial y}=x$。\n",
    "\n",
    "加法节点和乘法节点的计算图表示如下：\n",
    "\n",
    "![加法节点](http://www.ituring.com.cn/figures/2018/DeepLearning/067.png)\n",
    "![乘法节点](http://www.ituring.com.cn/figures/2018/DeepLearning/070.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 层的基本结构\n",
    "根据上一节中所列出的反向误差传播法，一个层应该包含一个正向传播方法和一个反向传播方法；通常还包含一个初始化的设置方法。因此，一个层的类可表示为：\n",
    "```\n",
    "class NetLayer:\n",
    "    # 初始化\n",
    "    def __init__(self):\n",
    "        # to be completed\n",
    "    \n",
    "    # 正向传播\n",
    "    def forward(self, x, y):\n",
    "        # to be completed\n",
    "    \n",
    "    # 反向传播\n",
    "    def backward(self, dout):\n",
    "        # to be completed\n",
    "```\n",
    "正向传播的方法参数不一定是两个也可能是1个或多个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各种层的实现\n",
    "\n",
    "### 激活层：ReLU层\n",
    "ReLU层的导数可表示为：\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = \\begin{cases}\n",
    "   1 &\\text{if } x > 0 \\\\\n",
    "   0 &\\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "对应的Python实现\n",
    "```\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "```\n",
    "\n",
    "### 激活层：sigmoid层\n",
    "sigmoid函数可表示为：\n",
    "$$\n",
    "y = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "则有\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial y}{\\partial x} &= \\frac{e^{-x}}{(1+e^{-x})^2} \\\\\n",
    "&=\\frac{1}{(1+e^{-x})}\\cdot\\frac{e^{-x}}{(1+e^{-x})} \\\\\n",
    "&=y(1-y)\n",
    "\\end{aligned}\n",
    "$$\n",
    "通过计算图逐步分解，也可以得到相同的结果。因此，可实现为：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 隐藏层：Affine层\n",
    "\n",
    "隐藏节点中的矩阵运算可抽象为Affine层，数学上可表示为：\n",
    "$$ \\mathbf{X} \\cdot \\mathbf{W} + \\mathbf{B} = \\mathbf{Y} $$\n",
    "因此，假设$L(\\mathbf{Y})$为损失函数，则有：\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{X}} = \\frac{\\partial L}{\\partial \\mathbf{Y}}\\cdot\\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{X}}=\\frac{\\partial L}{\\partial \\mathbf{Y}}\\cdot \\mathbf{W}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mathbf{W}} = \\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{W}}\\cdot\\frac{\\partial L}{\\partial \\mathbf{Y}}=\\mathbf{X}^T\\cdot\\frac{\\partial L}{\\partial \\mathbf{Y}} \n",
    "$$\n",
    "\n",
    "**为什么右乘变成了左乘？矩阵运算没有实际的除法，都是对乘法的另一种表达。**\n",
    "\n",
    "Python函数实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出层：Softmax-with-Loss\n",
    "**为什么按层处理时，Softmax一定要with Loss?**\n",
    "这里选择的损失函数为交叉熵函数。Softmax搭配交叉熵函数为损失函数，正好可以得到一个比较简单的反向传播函数。这使得层的计算变得简单。  \n",
    "如果损失函数不是交叉熵函数，而是均方误差函数，则输出层选择恒等函数，可以得到较为简单的层的传播。  \n",
    "计算过程不做过多阐述，最终层可以得到$(y_1-t_1,y_2-t_2,\\cdots,y_N-t_N)$这样得反向传输结果。 \n",
    "\n",
    "**而且，对于恒等函数搭配均方误差函数的输出层聚合，可以得到相同的反向传输结果**\n",
    "\n",
    "![image](http://www.ituring.com.cn/figures/2018/DeepLearning/091.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmax的输出\n",
    "        self.t = None # 监督数据\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
