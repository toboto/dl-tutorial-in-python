{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的组成方式\n",
    "将感知机和激活函数连接即可形成一层神经网络。  \n",
    "![image](http://www.ituring.com.cn/figures/2018/DeepLearning/022.png)\n",
    "将神经网络写成：\n",
    "\n",
    "$$ a = b + w_1x_1 + w_2x_2 \\\\ y = h(a) $$\n",
    "\n",
    "可以将常量$b$也引入到节点中，$w_1$和$w_2$表示对输入参数的权重计算，$b$表示该节点的触发灵敏度。  \n",
    "$h(a)$为激活函数，通过激活函数可以计算输入信号转换为输出信号，以判定输出为1或0。在神经网络的中间层节点中，激活函数返回的可能是连续的数值，而非直接的判定结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数\n",
    "### 阶跃函数\n",
    "当输入超过0时，输出1；否则，输出0。阶跃函数不是连续函数，是判定函数的理想形式\n",
    "### sigmoid函数\n",
    "sigmoid函数可表示为：\n",
    "\n",
    "$$\n",
    "h(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "sigmoid函数和阶跃函数形状很类似，但它是连续函数，除了可以用于判定，还可以表达实值信号的强度。在神经网络中，sigmoid函数具有重要的意义。\n",
    "\n",
    "### ReLU函数\n",
    "ReLU函数输出在输入大于0时为输入本身，在小雨等于0时输出等于0。\n",
    "$$\n",
    "h(x) = \\begin{cases}\n",
    "   x &\\text{if } x > 0 \\\\\n",
    "   0 &\\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "### 三种激活函数的比较\n",
    "\n",
    "![image](http://dev-pic.oss-cn-beijing.aliyuncs.com/python/hx.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多维数组运算\n",
    "多维数组运算即为矩阵运算，python中用来表示矩阵乘法的函数为\n",
    "```\n",
    "np.dot(A, B)\n",
    "```\n",
    "采用矩阵的表达方式，一层神经网络可以表示成：\n",
    "\n",
    "$$ \\mathbf{X} \\cdot \\mathbf{W} + \\mathbf{B} = \\mathbf{Y} $$\n",
    "其中\n",
    "$$ \\mathbf{X} = [x_1, x_2, \\cdots, x_N] $$ \n",
    "\n",
    "$$\n",
    "\\mathbf{W} = \\begin{bmatrix}\n",
    "\\boldsymbol{w_1}, \\boldsymbol{w_2}, \\cdots \\boldsymbol{w_M}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{B} = [b_1, b_2, \\cdots, b_M] \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{Y} = [y_1, y_2, \\cdots, y_M]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{w_k} = \\begin{bmatrix} w_{k} \\\\ w_{k2} \\\\ \\vdots \\\\ w_{kN} \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层神经网络的传递\n",
    "### 中间层表达\n",
    "每一级可以用如下公式表达的神经网络为一层，神经网络可以包含多层。\n",
    "$$\n",
    "\\mathbf{X} \\cdot \\mathbf{W} + \\mathbf{B} = \\mathbf{Y}\n",
    "$$\n",
    "\n",
    "通常将第$k$层神经网络的权重符号表达为$w^{(k)}_{ij}$，其中$i$表示第$k$层的神经元（本层输出节点），$j$表示$k-1$层的神经元（本层输入节点）。\n",
    "\n",
    "习惯上用$\\mathbf{A}^{(k)}$来替换$\\mathbf{Y}$。则第一层神经元可表达为：\n",
    "$$\n",
    "\\mathbf{A^{(1)}} = \\mathbf{X} \\cdot \\mathbf{W^{(1)}} + \\mathbf{B^{(1)}} \n",
    "$$\n",
    "获得$\\mathbf{A^{(1)}}$后，可以通过激活函数进行转换，得到：\n",
    "$$\n",
    "\\mathbf{Z^{(1)}} = h(\\mathbf{A^{(1)}})\n",
    "$$\n",
    "例如：$h(x)=sigmoid(x)$。\n",
    "$\\mathbf{Z^{(1)}}$可以作为下一层神经网络的输入信号。  \n",
    "以此类推，可得对于第k层神经网络：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "   \\mathbf{Z^{(k)}} &= h(\\mathbf{A^{(k)}}) \\\\\n",
    "   &=h \\left( \\mathbf{Z^{(k-1)}} \\cdot \\mathbf{W^{(k)}} + \\mathbf{B^{(k)}} \\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### 输出层表达\n",
    "最后一层，也就是输出层的激活函数，可能和隐藏层不同。一般地，回归问题可以使用恒等函数，二元分类问题可以使用 sigmoid 函数，多元分类问题可以使用 softmax 函数。通常用`$\\sigma() $`来表示输出层激活函数，而不是`$h()$`。\n",
    "#### 恒等函数\n",
    "恒等函数可表示为：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_function(x):\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### softmax函数\n",
    "softmax函数可表示为：\n",
    "$$\n",
    "y_k = \\frac{e^{a_k}}{\\displaystyle\\sum_{i=1}^n e^{a_i}}\n",
    "$$\n",
    "\n",
    "softmax函数对一组参数取指数，随后计算每一个非线性变化后的参数结果占总体的比例。\n",
    "代码上可以做如下实现：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### softmax函数溢出问题\n",
    "softmax函数要考虑溢出的问题，因为指数随着输入参数的变大会快速增大，最终可能会超过一个数值存储单元的最大数值。  \n",
    "由于指数函数的特性，我们可以将softmax函数的分子分母同时乘以一个常数，最终这个常数将变为指数上与输入参数求和的常数。如下所示：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_k &= \\frac{Ce^{a_k}}{C\\displaystyle\\sum_{i=1}^n e^{a_i}} \\\\\n",
    " &=\\frac{e^{(a_k+C')}}{\\displaystyle\\sum_{i=1}^n e^{(a_i+C')}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "如果取$C'= -1 \\times max(a_1, a_2, \\cdots, a_N)$，则可以将softmax函数中的指数值进行等比例缩减，且最大值为1（$e^0=1$），避免了溢出的问题。\n",
    "\n",
    "##### softmax函数的特征\n",
    "softmax函数任意一项总是介于0~1之间，且对所有输出项求和总是等于1。这个特征使得softmax函数可以被视为“概率”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手写数字识别示例\n",
    "这一示例主要展示了对于已有网络如何通过测试序列对网络性能做出判断。一般地，会将已知的序列分为训练序列和测试序列。训练序列用于生成网络参数，测试序列用于验证网络的准确性。在Python中成批进行矩阵计算，可以充分利用矩阵据算的特点，提高计算效率。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
